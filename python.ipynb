{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting london_hashtags.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile london_hashtags.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import powerlaw\n",
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "class london_hashtags:\n",
    "    def __init__(self,path):\n",
    "        df = pd.read_csv(path)#,nrows=200000)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        self.data = df\n",
    "        \n",
    "    def print_summary_stats(self):\n",
    "        print('# hashtag-days:',len(self.data.index))\n",
    "        print('date range:',self.data['date'].min(),self.data['date'].max())\n",
    "\n",
    "        s=self.data.groupby(['hashtag'],as_index=False).agg({'count':'sum'}).sort_values('count',ascending=False)\n",
    "        s.index=range(1,len(s.index)+1)\n",
    "\n",
    "        sorted_unique_dates = pd.Series(np.sort(self.data['date'].unique()))\n",
    "\n",
    "        diff=sorted_unique_dates.diff()\n",
    "        u=pd.Series(diff.unique()).dropna()\n",
    "        #only two different gap lengths betwen measurements (1 and 2 days)\n",
    "\n",
    "        print('maximum number of days between measurements:', u.iloc[1]/u.iloc[0])\n",
    "        print('number of occassions maximum gap occurs:', len(diff[diff==u.iloc[1]]))\n",
    "\n",
    "        missing_dates = sorted_unique_dates[np.where(diff==u.iloc[1])[0]]\n",
    "\n",
    "        #sorted_unique_dates[sorted_unique_dates.isin(missing_dates - pd.Timedelta(1,'d'))]\n",
    "        print('missing dates are: ', [m - pd.Timedelta(1,'d') for m in missing_dates])\n",
    "        \n",
    "        \n",
    "    def derive_turnover_matrix(self,list_size=100,num_increments=21):\n",
    "        \n",
    "        #add list features as attributes\n",
    "        self.list_size = list_size\n",
    "        self.num_increments = num_increments\n",
    "        \n",
    "        def nlargestGroup(df,list_size):\n",
    "            return (df.nlargest(list_size,'count').\n",
    "                    assign(rank = lambda df: df['count'].rank(method='first',ascending = False)))\n",
    "\n",
    "        def turnover(df):\n",
    "            return len(df[~df['hashtag_today'].isin(df['hashtag_yesterday'])].index)\n",
    "\n",
    "        def today_yesterday_merge(df):\n",
    "            return (df.merge(df, left_on=['date','rank'], right_on=['yesterday','rank'], \n",
    "                  suffixes = ('_today','_yesterday'),how='inner'))\n",
    "\n",
    "        def get_turnover_matrix(df,list_size,num_increments):\n",
    "            turnover_df = pd.DataFrame()\n",
    "            \n",
    "            for ls in np.linspace(1,list_size,num_increments,dtype='int'): \n",
    "                turnover_df[ls]=df[df['rank'] <= ls].groupby('date_today').apply(turnover)\n",
    "            \n",
    "            return turnover_df\n",
    "        \n",
    "        self.turnover_matrix=(\n",
    "            self.data.groupby('date',as_index=False).apply(nlargestGroup,list_size).\n",
    "            assign(yesterday = lambda df: df['date']-pd.Timedelta(1,'d')).\n",
    "            pipe(today_yesterday_merge).\n",
    "            pipe(get_turnover_matrix,list_size,num_increments)\n",
    "        ) \n",
    "        \n",
    "        return self.turnover_matrix\n",
    "    \n",
    "    def get_daily_sample_size(self): \n",
    "        self.daily_sample_size = self.data.groupby('date').agg({'count':'count'})\n",
    "        return self.daily_sample_size\n",
    "    \n",
    "    def get_powerlaw_fit(self): \n",
    "        \n",
    "        #hashtag counts over all dates\n",
    "        self.all_time_rankings = (self.data.groupby('hashtag').agg({'count':'sum'})['count'].\n",
    "                                sort_values(ascending = False))\n",
    "        \n",
    "        # fit power law \n",
    "        self.powerlaw_fit=powerlaw.Fit(self.all_time_rankings,verbose=False)\n",
    "        self.alpha = self.powerlaw_fit.alpha #power law exponent\n",
    "        \n",
    "        return self.all_time_rankings\n",
    "    \n",
    "    def get_daily_alpha(self):\n",
    "\n",
    "        def calculate_daily_powerlaw(df):\n",
    "            fit=powerlaw.Fit(df['count'],verbose=False)\n",
    "            return pd.Series([fit.alpha, fit.sigma],index=['alpha','sigma'])\n",
    "\n",
    "        df2=self.data.groupby('date').apply(calculate_daily_powerlaw)\n",
    "\n",
    "        self.daily_alpha = df2['alpha']\n",
    "        self.daily_sigma = df2['sigma'] #also collect the daily statistical errors\n",
    "\n",
    "        return self.daily_alpha,self.daily_sigma\n",
    "    \n",
    "    \n",
    "    ### indivual methods for the plots ####\n",
    "    \n",
    "    def plotaxis_turnover_by_listsize(self,ax):\n",
    "    \n",
    "        if hasattr(self,'turnover_matrix') == False:\n",
    "            #set list size and increments to default\n",
    "            list_size=100\n",
    "            num_increments=21\n",
    "            self.derive_turnover_matrix(list_size,num_increments)\n",
    "\n",
    "\n",
    "        list_vec = self.turnover_matrix.columns\n",
    "        mean = self.turnover_matrix.mean()\n",
    "        error = self.turnover_matrix.std()\n",
    "\n",
    "        #f, ax = plt.subplots()\n",
    "\n",
    "        ax.plot(list_vec,mean)\n",
    "        ax.fill_between(list_vec,mean-error,mean+error,alpha=0.5)\n",
    "        ax.set_xlabel('list size',fontsize=16)\n",
    "        ax.set_ylabel('turnover',fontsize=16)\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plotaxis_turnover_by_time(self,ax):\n",
    "\n",
    "        if hasattr(self,'turnover_matrix') == False:\n",
    "            #set list size and increments to default\n",
    "            list_size=100\n",
    "            num_increments=21\n",
    "            self.derive_turnover_matrix(list_size,num_increments)\n",
    "\n",
    "        #HARD WIRED DATE TICKS\n",
    "        ticks_to_use = pd.Series([pd.to_datetime('2017'),pd.to_datetime('2018'),\n",
    "                                  pd.to_datetime('2019'),pd.to_datetime('2020')])\n",
    "\n",
    "        labels = ticks_to_use.dt.year\n",
    "\n",
    "        ax.plot(self.turnover_matrix[self.list_size],alpha=0.7)\n",
    "\n",
    "        # Now set the ticks and labels\n",
    "        ax.set_xticks(ticks_to_use)\n",
    "        ax.set_xticklabels(labels)\n",
    "\n",
    "        ax.set_xlabel('year',fontsize=16)\n",
    "        ax.set_ylabel('turnover (top ' + str(self.list_size) + ')',fontsize=16)\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plotaxis_sample_size(self,ax):\n",
    "\n",
    "        if hasattr(self,'daily_sample_size') == False:\n",
    "            self.get_daily_sample_size()\n",
    "\n",
    "        #HARD WIRED DATE TICKS\n",
    "        ticks_to_use = pd.Series([pd.to_datetime('2017'),pd.to_datetime('2018'),\n",
    "                                  pd.to_datetime('2019'),pd.to_datetime('2020')])\n",
    "\n",
    "        labels = ticks_to_use.dt.year\n",
    "\n",
    "        ax.plot(self.daily_sample_size.index,self.daily_sample_size,'-')\n",
    "        ax.set_ylabel('sample size',fontsize=16)\n",
    "        ax.set_xlabel('year',fontsize=16)\n",
    "        ax.set_xticks(ticks_to_use)\n",
    "        ax.set_xticklabels(labels)\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plotaxis_turnover_by_sample_size(self,ax):\n",
    "\n",
    "        if hasattr(self,'daily_sample_size') == False:\n",
    "            self.get_daily_sample_size()\n",
    "\n",
    "        if hasattr(self,'turnover_matrix') == False:\n",
    "            #set list size and increments to default\n",
    "            list_size=100\n",
    "            num_increments=21\n",
    "            self.derive_turnover_matrix(list_size,num_increments)\n",
    "\n",
    "        x=self.turnover_matrix[self.list_size]\n",
    "        y=self.daily_sample_size.loc[self.turnover_matrix.index,'count']\n",
    "\n",
    "        lowess = sm.nonparametric.lowess(endog=y, exog=x, frac=0.9)    \n",
    "        xfit,yfit = list(zip(*lowess))\n",
    "\n",
    "        ax.plot(x,y,'o',alpha=0.7,label='_legend_')\n",
    "        ax.plot(xfit,yfit,'-',c='darkred',lw=4,alpha=0.7,label='LOESS fit')\n",
    "        ax.set_ylabel('sample size',fontsize=16)\n",
    "        ax.set_xlabel('turnover (top 100)',fontsize=16)\n",
    "        ax.legend(fontsize=14)\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plotaxis_powerlaw(self,ax):\n",
    "\n",
    "        if hasattr(self,'powerlaw_fit') == False:\n",
    "            self.get_powerlaw_fit()\n",
    "\n",
    "        def predicted_y(x,fit):\n",
    "            yfit = x**-fit.alpha\n",
    "            yfit = yfit/sum(yfit)\n",
    "            return yfit\n",
    "\n",
    "        # power law histogram of hashtags\n",
    "        x,y = self.powerlaw_fit.pdf()\n",
    "        x = x[:-1] + np.diff(x)/2\n",
    "        y = y/sum(y) #normlize probability density\n",
    "\n",
    "        #power law line of best fit\n",
    "        yfit = predicted_y(x,self.powerlaw_fit)\n",
    "\n",
    "        ax.loglog(x,y,'s',lw=3,label='data histogram')\n",
    "        ax.loglog(x,yfit,'--',lw=2,c='darkred',label=r'power law fit ($\\alpha$= ' + str(np.round(self.alpha,2)) + ')')\n",
    "\n",
    "        ax.set_xlabel('hashtag frequency',fontsize=16)\n",
    "        ax.set_ylabel('probability',fontsize=16)\n",
    "\n",
    "        ax.legend(fontsize=14)\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plotaxis_daily_powerlawalpha(self,ax):\n",
    "\n",
    "        if hasattr(self,'daily_alpha') == False:\n",
    "            self.get_daily_alpha()\n",
    "\n",
    "        date_marks = pd.to_datetime(pd.Series(['2017','2018','2019','2020']))\n",
    "        date_labels = date_marks.dt.year\n",
    "\n",
    "        ax.plot(self.daily_alpha.index,self.daily_alpha,label=r'$\\alpha$')\n",
    "        ax.fill_between(self.daily_alpha.index, \n",
    "                        self.daily_alpha-self.daily_sigma, \n",
    "                        self.daily_alpha+self.daily_sigma, alpha=0.5,label='std. err.')\n",
    "\n",
    "\n",
    "        ax.set_xticks(date_marks)\n",
    "        ax.set_xticklabels(date_labels)\n",
    "        ax.set_ylabel(r'$\\alpha$',fontsize=20,rotation=0)\n",
    "        ax.set_xlabel('year',fontsize=16)\n",
    "\n",
    "        ax.legend(fontsize=14)\n",
    "\n",
    "        return ax\n",
    "    \n",
    "    def summary_plot(self,save_directory):\n",
    "        print(save_directory)\n",
    "\n",
    "        plt.style.use('tableau-colorblind10')\n",
    "\n",
    "        f, ax = plt.subplots(3,2,figsize=[13,9])\n",
    "\n",
    "        self.plotaxis_turnover_by_listsize(ax[0,0])\n",
    "        self.plotaxis_turnover_by_time(ax[0,1])\n",
    "        self.plotaxis_sample_size(ax[1,0])\n",
    "        self.plotaxis_turnover_by_sample_size(ax[1,1])\n",
    "        self.plotaxis_powerlaw(ax[2,0])\n",
    "        self.plotaxis_daily_powerlawalpha(ax[2,1])\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "\n",
    "        plt.savefig(save_directory+'summary_plots.pdf')\n",
    "    ###end plots ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_london_hashtags.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 'run_london_hashtags.py'\n",
    "\n",
    "import london_hashtags\n",
    "\n",
    "path = 'data/hashtagCountsPerDayLondon.csv'\n",
    "\n",
    "list_size=100\n",
    "num_increments=21\n",
    "\n",
    "self = london_hashtags.london_hashtags(path)\n",
    "self.print_summary_stats()\n",
    "\n",
    "\n",
    "turnover = self.derive_turnover_matrix(list_size,num_increments)\n",
    "Nt = self.get_daily_sample_size()\n",
    "daily_sample_size=self.get_daily_sample_size()\n",
    "powerlaw_fit=self.get_powerlaw_fit()\n",
    "daily_alpha,daily_sigma=self.get_daily_alpha()\n",
    "\n",
    "self.summary_plot('plots/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    13814.435897\n",
       " dtype: float64, count    8375.461656\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outlier alpha day, get sample size\n",
    "outlier_alpha_date = self.daily_alpha.idxmax()\n",
    "self.daily_sample_size.loc[outlier_alpha_date]\n",
    "\n",
    "\n",
    "#sample size before and after Jan 1st 2018\n",
    "s = self.daily_sample_size\n",
    "s.loc[:pd.to_datetime('2017-12-31')].mean(),s.loc[pd.to_datetime('2017-12-31'):].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
