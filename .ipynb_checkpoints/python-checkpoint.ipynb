{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting london_hashtags.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile london_hashtags.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import powerlaw\n",
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import datetime\n",
    "\n",
    "class london_hashtags:\n",
    "    def __init__(self,path,start_date):\n",
    "        self.minimum_daily_sample_size = 4000 #default sample size cutoff\n",
    "        \n",
    "        df = pd.read_csv(path)#,nrows=200000)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        #include only dates after 'start_date'\n",
    "        df = df[df['date'] >= start_date]\n",
    "        self.data = df\n",
    "        \n",
    "    def print_summary_stats(self):\n",
    "        print('# hashtag-days:',len(self.data.index))\n",
    "        print('date range:',self.data['date'].min(),self.data['date'].max())\n",
    "\n",
    "        s=self.data.groupby(['hashtag'],as_index=False).agg({'count':'sum'}).sort_values('count',ascending=False)\n",
    "        s.index=range(1,len(s.index)+1)\n",
    "\n",
    "        sorted_unique_dates = pd.Series(np.sort(self.data['date'].unique()))\n",
    "\n",
    "        diff=sorted_unique_dates.diff()\n",
    "        u=pd.Series(diff.unique()).dropna()\n",
    "        #only two different gap lengths betwen measurements (1 and 2 days)\n",
    "\n",
    "        print('maximum number of days between measurements:', u.iloc[1]/u.iloc[0])\n",
    "        print('number of occassions maximum gap occurs:', len(diff[diff==u.iloc[1]]))\n",
    "\n",
    "        missing_dates = sorted_unique_dates[np.where(diff==u.iloc[1])[0]]\n",
    "\n",
    "        #sorted_unique_dates[sorted_unique_dates.isin(missing_dates - pd.Timedelta(1,'d'))]\n",
    "        print('missing dates are: ', [m - pd.Timedelta(1,'d') for m in missing_dates])\n",
    "        \n",
    "        \n",
    "    def derive_turnover_matrix(self,list_size=100,num_increments=21):\n",
    "        \n",
    "        #add list features as attributes\n",
    "        self.list_size = list_size\n",
    "        self.num_increments = num_increments\n",
    "        \n",
    "        def nlargestGroup(df,list_size):\n",
    "            return (df.nlargest(list_size,'count').\n",
    "                    assign(rank = lambda df: df['count'].rank(method='first',ascending = False)))\n",
    "\n",
    "        def turnover(df):\n",
    "            return len(df[~df['hashtag_today'].isin(df['hashtag_yesterday'])].index)\n",
    "\n",
    "        def today_yesterday_merge(df):\n",
    "            return (df.merge(df, left_on=['date','rank'], right_on=['yesterday','rank'], \n",
    "                  suffixes = ('_today','_yesterday'),how='inner'))\n",
    "\n",
    "        def get_turnover_matrix(df,list_size,num_increments):\n",
    "            turnover_df = pd.DataFrame()\n",
    "            \n",
    "            for ls in np.linspace(1,list_size,num_increments,dtype='int'): \n",
    "                turnover_df[ls]=df[df['rank'] <= ls].groupby('date_today').apply(turnover)\n",
    "            \n",
    "            return turnover_df\n",
    "        \n",
    "        self.turnover_matrix=(\n",
    "            self.data.groupby('date',as_index=False).apply(nlargestGroup,list_size).\n",
    "            assign(yesterday = lambda df: df['date']-pd.Timedelta(1,'d')).\n",
    "            pipe(today_yesterday_merge).\n",
    "            pipe(get_turnover_matrix,list_size,num_increments)\n",
    "        ) \n",
    "        \n",
    "        return self.turnover_matrix\n",
    "    \n",
    "    def get_daily_sample_size(self): \n",
    "        self.daily_sample_size = self.data.groupby('date').agg({'count':'count'})\n",
    "        return self.daily_sample_size\n",
    "    \n",
    "    def get_powerlaw_fit(self): \n",
    "        \n",
    "        #hashtag counts over all dates\n",
    "        self.all_time_rankings = (self.data.groupby('hashtag').agg({'count':'sum'})['count'].\n",
    "                                sort_values(ascending = False))\n",
    "        \n",
    "        # fit power law \n",
    "        self.powerlaw_fit=powerlaw.Fit(self.all_time_rankings,verbose=False)\n",
    "        self.alpha = self.powerlaw_fit.alpha #power law exponent\n",
    "        \n",
    "        return self.all_time_rankings\n",
    "    \n",
    "    def get_daily_alpha(self):\n",
    "\n",
    "        def calculate_daily_powerlaw(df):\n",
    "            fit=powerlaw.Fit(df['count'],verbose=False)\n",
    "            return pd.Series([fit.alpha, fit.sigma],index=['alpha','sigma'])\n",
    "\n",
    "        df2=self.data.groupby('date').apply(calculate_daily_powerlaw)\n",
    "\n",
    "        self.daily_alpha = df2['alpha']\n",
    "        self.daily_sigma = df2['sigma'] #also collect the daily statistical errors\n",
    "\n",
    "        return self.daily_alpha,self.daily_sigma\n",
    "    \n",
    "    \n",
    "    ### indivual methods for the plots ####\n",
    "    \n",
    "    def plotaxis_turnover_by_listsize(self,ax):\n",
    "    \n",
    "        if hasattr(self,'turnover_matrix') == False:\n",
    "            #set list size and increments to default\n",
    "            list_size=100\n",
    "            num_increments=21\n",
    "            self.derive_turnover_matrix(list_size,num_increments)\n",
    "\n",
    "\n",
    "        list_vec = self.turnover_matrix.columns\n",
    "        mean = self.turnover_matrix.mean()\n",
    "        error = self.turnover_matrix.std()\n",
    "\n",
    "        #f, ax = plt.subplots()\n",
    "\n",
    "        ax.plot(list_vec,mean)\n",
    "        ax.fill_between(list_vec,mean-error,mean+error,alpha=0.5)\n",
    "        ax.set_xlabel('list size',fontsize=16)\n",
    "        ax.set_ylabel('turnover',fontsize=16)\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plotaxis_turnover_by_time(self,ax):\n",
    "\n",
    "        if hasattr(self,'turnover_matrix') == False:\n",
    "            #set list size and increments to default\n",
    "            list_size=100\n",
    "            num_increments=21\n",
    "            self.derive_turnover_matrix(list_size,num_increments)\n",
    "\n",
    "        #HARD WIRED DATE TICKS\n",
    "        ticks_to_use = pd.Series([pd.to_datetime('2017'),pd.to_datetime('2018'),\n",
    "                                  pd.to_datetime('2019'),pd.to_datetime('2020')])\n",
    "\n",
    "        labels = ticks_to_use.dt.year\n",
    "\n",
    "        ax.plot(self.turnover_matrix[self.list_size],alpha=0.7)\n",
    "\n",
    "        # Now set the ticks and labels\n",
    "        ax.set_xticks(self.date_marks)\n",
    "        ax.set_xticklabels(self.date_labels)\n",
    "\n",
    "        ax.set_xlabel('year',fontsize=16)\n",
    "        ax.set_ylabel('turnover (top ' + str(self.list_size) + ')',fontsize=16)\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plotaxis_sample_size(self,ax):\n",
    "\n",
    "        if hasattr(self,'daily_sample_size') == False:\n",
    "            self.get_daily_sample_size()\n",
    "            \n",
    "            \n",
    "        ax.plot(self.daily_sample_size.index,self.daily_sample_size,'-')\n",
    "        ax.set_ylabel('sample size',fontsize=16)\n",
    "        ax.set_xlabel('year',fontsize=16)\n",
    "        ax.set_xticks(self.date_marks)\n",
    "        ax.set_xticklabels(self.date_labels)\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plotaxis_turnover_by_sample_size(self,ax):\n",
    "\n",
    "        if hasattr(self,'daily_sample_size') == False:\n",
    "            self.get_daily_sample_size()\n",
    "\n",
    "        if hasattr(self,'turnover_matrix') == False:\n",
    "            #set list size and increments to default\n",
    "            list_size=100\n",
    "            num_increments=21\n",
    "            self.derive_turnover_matrix(list_size,num_increments)\n",
    "\n",
    "        x=self.turnover_matrix[self.list_size]\n",
    "        y=self.daily_sample_size.loc[self.turnover_matrix.index,'count']\n",
    "\n",
    "        lowess = sm.nonparametric.lowess(endog=y, exog=x, frac=0.9)    \n",
    "        xfit,yfit = list(zip(*lowess))\n",
    "\n",
    "        ax.plot(x,y,'o',alpha=0.7,label='_legend_')\n",
    "        ax.plot(xfit,yfit,'-',c='darkred',lw=4,alpha=0.7,label='LOESS fit')\n",
    "        ax.set_ylabel('sample size',fontsize=16)\n",
    "        ax.set_xlabel('turnover (top 100)',fontsize=16)\n",
    "        ax.legend(fontsize=14)\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plotaxis_powerlaw(self,ax):\n",
    "\n",
    "        if hasattr(self,'powerlaw_fit') == False:\n",
    "            self.get_powerlaw_fit()\n",
    "\n",
    "        def predicted_y(x,fit):\n",
    "            yfit = x**-fit.alpha\n",
    "            yfit = yfit/sum(yfit)\n",
    "            return yfit\n",
    "\n",
    "        # power law histogram of hashtags\n",
    "        x,y = self.powerlaw_fit.pdf()\n",
    "        x = x[:-1] + np.diff(x)/2\n",
    "        y = y/sum(y) #normlize probability density\n",
    "\n",
    "        #power law line of best fit\n",
    "        yfit = predicted_y(x,self.powerlaw_fit)\n",
    "\n",
    "        ax.loglog(x,y,'s',lw=3,label='data histogram')\n",
    "        ax.loglog(x,yfit,'--',lw=2,c='darkred',label=r'power law fit ($\\alpha$= ' + str(np.round(self.alpha,2)) + ')')\n",
    "\n",
    "        ax.set_xlabel('hashtag frequency',fontsize=16)\n",
    "        ax.set_ylabel('probability',fontsize=16)\n",
    "\n",
    "        ax.legend(fontsize=14)\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def plotaxis_daily_powerlawalpha(self,ax):\n",
    "\n",
    "        if hasattr(self,'daily_alpha') == False:\n",
    "            self.get_daily_alpha()\n",
    "\n",
    "\n",
    "        ax.plot(self.daily_alpha.index,self.daily_alpha,label=r'$\\alpha$')\n",
    "        ax.fill_between(self.daily_alpha.index, \n",
    "                        self.daily_alpha-self.daily_sigma, \n",
    "                        self.daily_alpha+self.daily_sigma, alpha=0.5,label='std. err.')\n",
    "\n",
    "\n",
    "        ax.set_xticks(self.date_marks)\n",
    "        ax.set_xticklabels(self.date_labels)\n",
    "        ax.set_ylabel(r'$\\alpha$',fontsize=20,rotation=0)\n",
    "        ax.set_xlabel('year',fontsize=16)\n",
    "\n",
    "        ax.legend(fontsize=14)\n",
    "\n",
    "        return ax\n",
    "    \n",
    "    def summary_plot(self,save_directory):\n",
    "        print(save_directory)\n",
    "\n",
    "        plt.style.use('tableau-colorblind10')\n",
    "\n",
    "        f, ax = plt.subplots(3,2,figsize=[13,9])\n",
    "\n",
    "        self.plotaxis_turnover_by_listsize(ax[0,0])\n",
    "        self.plotaxis_turnover_by_time(ax[0,1])\n",
    "        self.plotaxis_sample_size(ax[1,0])\n",
    "        self.plotaxis_turnover_by_sample_size(ax[1,1])\n",
    "        self.plotaxis_powerlaw(ax[2,0])\n",
    "        self.plotaxis_daily_powerlawalpha(ax[2,1])\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "\n",
    "        plt.savefig(save_directory+'summary_plots.pdf')\n",
    "        \n",
    "    ###end plots ######\n",
    "    \n",
    "    def date_ticks(self,list_of_years):\n",
    "        self.date_marks = pd.to_datetime(pd.Series(list_of_years))\n",
    "        self.date_labels = self.date_marks.dt.year\n",
    "        \n",
    "    def remove_low_sample_days(self,dates_to_remove=None):\n",
    "        if dates_to_remove is None:# if no preset dates to remove\n",
    "            print('nosh')\n",
    "            prefiltering_daily_sample_size = self.get_daily_sample_size() \n",
    "            dates_to_remove = prefiltering_daily_sample_size[(prefiltering_daily_sample_size > self.minimum_daily_sample_size)['count']].index\n",
    "\n",
    "        self.data=self.data[self.data['date'].isin(dates_to_remove)]\n",
    "        \n",
    "        \n",
    "    def graphically_choose_new_sample_size_cutoff(self):\n",
    "        \n",
    "        prefiltering_daily_sample_size = self.get_daily_sample_size() \n",
    "\n",
    "        plt.style.use('tableau-colorblind10')\n",
    "        f, ax  = plt.subplots()\n",
    "\n",
    "        ax.plot(prefiltering_daily_sample_size['count'].sort_values().values,'o')\n",
    "        ax.set_ylabel('sample size',fontsize=16)\n",
    "        ax.set_xlabel('day rank',fontsize=16)\n",
    "\n",
    "\n",
    "        ax.set_title('Choose a minimum sample size cutoff?',fontsize=16)\n",
    "        xy = plt.ginput(1)\n",
    "        self.minimum_daily_sample_size = xy[0][1]\n",
    "        print('new minimum sample size:' + str(np.round(self.minimum_daily_sample_size,0)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'high_sample_dates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-314f01ed96d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# do you want to remove days with small sample sizes?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#self.graphically_choose_new_sample_size_cutoff()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_low_sample_days\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates_to_remove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#set x ticks for date axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/LondonHashtags/london_hashtags.py\u001b[0m in \u001b[0;36mremove_low_sample_days\u001b[0;34m(self, dates_to_remove)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates_to_remove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgraphically_choose_new_sample_size_cutoff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'high_sample_dates' is not defined"
     ]
    }
   ],
   "source": [
    "#%%writefile 'run_london_hashtags.py'\n",
    "\n",
    "import london_hashtags\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "path = 'data/hashtagCountsPerDayLondon.csv'\n",
    "start_date = pd.to_datetime('01/01/2018')\n",
    "\n",
    "\n",
    "self = london_hashtags.london_hashtags(path,start_date)\n",
    "#elf = london_hashtags(path,start_date)\n",
    "\n",
    "list_size=100\n",
    "num_increments=21\n",
    "\n",
    "#dates with number of tweets less than 2.5 standard deviations below mean \n",
    "dates_to_remove = pd.to_datetime(['2018-05-21', '2018-05-22', '2018-08-09', '2018-11-30','2019-07-11', '2019-08-28', \n",
    "                '2019-08-29', '2019-08-31', '2019-09-11', '2019-09-12', '2019-09-13'])\n",
    "\n",
    "# do you want to remove days with small sample sizes?\n",
    "#self.graphically_choose_new_sample_size_cutoff()\n",
    "self.remove_low_sample_days(dates_to_remove)\n",
    "\n",
    "#set x ticks for date axes\n",
    "list_of_years = ['2018','2019','2020']\n",
    "self.date_ticks(list_of_years)\n",
    "\n",
    "\n",
    "## needs to be made more general\n",
    "#self.print_summary_stats()\n",
    "\n",
    "turnover = self.derive_turnover_matrix(list_size,num_increments)\n",
    "Nt = self.get_daily_sample_size()\n",
    "daily_sample_size=self.get_daily_sample_size()\n",
    "powerlaw_fit=self.get_powerlaw_fit()\n",
    "daily_alpha,daily_sigma=self.get_daily_alpha()\n",
    "\n",
    "self.summary_plot('plots/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nosh\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLR = -9.4, p value = 0.42, most likely distribution is lognormal_positive\n"
     ]
    }
   ],
   "source": [
    "def \n",
    "\n",
    "\n",
    "dist1='power_law'\n",
    "dist2='lognormal_positive'\n",
    "\n",
    "R,p = self.powerlaw_fit.distribution_compare(dist1,dist2)\n",
    "\n",
    "if R > 0: most_likely_dist = dist1\n",
    "if R <= 0: most_likely_dist = dist2\n",
    "\n",
    "print('LLR = ' + str(np.round(R,2)) + ', p value = ' + str(np.round(p,2)) + ', most likely distribution is ' + most_likely_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powerlaw.loglikelihood_ratio(loglikelihoods1, loglikelihoods2, nested=False, normalized_ratio=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "log_normal_positive",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f60e1217b8e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpowerlaw_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'power_law'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'log_normal_positive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/powerlaw.py\u001b[0m in \u001b[0;36mdistribution_compare\u001b[0;34m(self, dist1, dist2, nested, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mdist1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mdist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mloglikelihoods1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloglikelihoods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/powerlaw.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_xmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxmin_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: log_normal_positive"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    13814.435897\n",
       " dtype: float64, count    8375.461656\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outlier alpha day, get sample size\n",
    "outlier_alpha_date = self.daily_alpha.idxmax()\n",
    "self.daily_sample_size.loc[outlier_alpha_date]\n",
    "\n",
    "\n",
    "#sample size before and after Jan 1st 2018\n",
    "s = self.daily_sample_size\n",
    "s.loc[:pd.to_datetime('2017-12-31')].mean(),s.loc[pd.to_datetime('2017-12-31'):].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
